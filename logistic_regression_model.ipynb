{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 8 Assignment Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://imac.modem:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591169107792)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder, PCA}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, Multic..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Start a simple Spark Session\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "//Feature Processing Classes\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder, PCA}\n",
    "\n",
    "//Linear Algebra Data Structures\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "\n",
    "//Model Building Pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "//Binary Classification\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel,\n",
    "                                           RandomForestClassifier, GBTClassifier,\n",
    "                                           DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
    "//Model Training\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, \n",
    "                                   ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "//Model Evaluation\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "\n",
    "//Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "//For Cleaning\n",
    "//import scala.util.matching.Regex\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Group 8 ML Phase 3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a parquet file of flight delay, fuel-price and meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Departing_Port: string (nullable = true)\n",
      " |-- Arriving_Port: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month_Num: string (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- Departing_Port_station_ID: string (nullable = true)\n",
      " |-- Departing_Port_station_name: string (nullable = true)\n",
      " |-- Arriving_Port_station_ID: string (nullable = true)\n",
      " |-- Arriving_Port_station_name: string (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Depart: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Depart: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Depart: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Depart: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Depart: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Depart: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Arrive: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Arrive: double (nullable = true)\n",
      " |-- Date_Num: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flights: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights = (spark\n",
    "            .read.parquet(\"flightDelay.parquet\")\n",
    "            .withColumn(\"Month_Num1\", $\"Month_Num\" cast \"Int\")\n",
    "            //convert month and year to integer index starting Jan 2004\n",
    "            .withColumn(\"Date_Num\",  ($\"Year\"-2004)*12 + $\"Month_Num1\")\n",
    "            .drop(\"Sectors_Flown\", \"Month_Num1\", \"Change\")\n",
    "            .withColumnRenamed(\"Departures_Delayed\",\"label\")\n",
    "            .withColumnRenamed(\"Price\",\"Fuel_Price\")\n",
    "            //drop NA's even though none were found!\n",
    "            .na.drop()\n",
    "            //.cache\n",
    "              )\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the degree of imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts = flights.groupBy(\"label\").count()\n",
    "\n",
    "println(\"proportion of lates (label=1) in the sample\")\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split The Data into training and testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set of the Most Recent 12 Months has 435479 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "rawTraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out the most recent 12 months of flight data as the test dataset\n",
    "//Dates after March 2019 have Date_Num > 183\n",
    "val testing = flights.filter($\"Date_Num\"> 183)\n",
    "println(s\"Test Set of the Most Recent 12 Months has ${testing.count()} records\")\n",
    "\n",
    "//Filter out rows prior to the most recent 12 months of flight data as the training dataset\n",
    "val rawTraining = flights.filter($\"Date_Num\" < 184)\n",
    "\n",
    "//val Array(imbalancedTraining, testing) = flights.randomSplit(Array(0.7, 0.3), seed = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down sample the Ontime Departures To Balance The Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On time Training Flights: 4884963\n",
      "Delayed Training Flights: 976455\n",
      "Down Sampled ontime Training Flights: 976650\n",
      "proportion of lates (label=1) in the sample\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|97602|\n",
      "|    0|97793|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ontimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "delayedTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampledOntimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampleFraction: Double = 0.1\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "resampledCounts: org.apache.spark.sql.DataFrame = [label: int, count: bigint]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ontimeTrainingFlights = rawTraining.filter($\"label\"===0)\n",
    "println(s\"On time Training Flights: ${ontimeTrainingFlights.count()}\")\n",
    "\n",
    "val delayedTrainingFlights = rawTraining.filter($\"label\"===1)\n",
    "println(s\"Delayed Training Flights: ${delayedTrainingFlights.count()}\")\n",
    "\n",
    "val sampledOntimeTrainingFlights = ontimeTrainingFlights.sample(false, 0.2)  \n",
    "\n",
    "println(s\"Down Sampled ontime Training Flights: ${sampledOntimeTrainingFlights.count()}\")\n",
    "\n",
    "val sampleFraction = 0.1\n",
    "//Concatenate rows of ontimeTrainingFlights and delayedTrainingFlights\n",
    "val training = (sampledOntimeTrainingFlights\n",
    "                .union(delayedTrainingFlights)\n",
    "                .sample(false, sampleFraction))\n",
    "               \n",
    "val resampledCounts = training.groupBy(\"label\").count()\n",
    "println(\"proportion of lates (label=1) in the sample\")\n",
    "resampledCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment via a Confusion Matrix Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getConfusionMatrix: (predictionDF: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getConfusionMatrix(predictionDF: DataFrame): Unit = {\n",
    "    val eval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "    println(s\"Accuracy: ${eval.setMetricName(\"accuracy\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Precision: ${eval.setMetricName(\"weightedPrecision\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Recall: ${eval.setMetricName(\"weightedRecall\").evaluate(predictionDF)}\")\n",
    "    println(s\"F1: ${eval.setMetricName(\"f1\").evaluate(predictionDF)}\")\n",
    "\n",
    "    val TP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "    val TN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "    val FP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "    val FN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "    val total = predictionDF.select(\"label\").count.toDouble\n",
    "    // Unweighted Metrics\n",
    "    val accuracy    = (TP + TN) / total\n",
    "    val precision   = (TP + FP) / total\n",
    "    val recall      = (TP + FN) / total\n",
    "    val F1 = 2/(1/precision + 1/recall)\n",
    "    println(s\"Accuracy: ${accuracy}\")\n",
    "    println(s\"Precision: ${precision}\")\n",
    "    println(s\"Recall: ${recall}\")\n",
    "    println(s\"F1: ${F1}\")\n",
    "\n",
    "    // Confusion matrix\n",
    "    printf(s\"\"\"|=================== Confusion Matrix ==========================\n",
    "           |##########| %-15s                     %-15s\n",
    "           |----------+----------------------------------------------------\n",
    "           |Actual = 0| %-15d                     %-15d\n",
    "           |Actual = 1| %-15d                     %-15d\n",
    "           |===============================================================\n",
    "         \"\"\".stripMargin, \"Predicted = 0\", \"Predicted = 1\", TN, FP, FN, TP)\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the ML Pipleline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      " aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true, current: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoricalVariables: Array[String] = Array(Departing_Port, Arriving_Port, Airline)\n",
       "categoricalIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_5fed8308a072, strIdx_1ca06f8e2bb6, strIdx_fcaded19f502)\n",
       "categoricalEncoders: Array[org.apache.spark.ml.feature.OneHotEncoder] = Array(oneHot_01b0873b8d15, oneHot_197ce11a5b17, oneHot_1941a45672db)\n",
       "cols: Array[String] = Array(Date_Num, Airline_Vec, Fuel_Price, Departing_Port_Vec, Mean_daily_wind_run_km_Depart, Mean_rainfall_mm_Depart, Mean_number_of_days_of_rain_Depart, Mean_number_of_days_>_40_Degrees_C_Depart, Arriving_Port_Vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8a50dea4dfeb\n",
       "pca: org.apache.spark.ml.feature.PCA = pca_ffb7754cf879\n",
       "lr: org.apache.spark.ml.classification.LogisticRegressi..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//////////////////////////////////////////////////\n",
    "//// Setting Up DataFrame for Machine Learning ///\n",
    "//////////////////////////////////////////////////\n",
    "\n",
    "// Deal with Categorical Columns\n",
    "val categoricalVariables = Array(\n",
    "    \"Departing_Port\", \"Arriving_Port\", \"Airline\")\n",
    "val categoricalIndexers = categoricalVariables\n",
    "  .map(i => new StringIndexer().setInputCol(i).setOutputCol(i+\"_Index\"))\n",
    "val categoricalEncoders = categoricalVariables\n",
    "  .map(e => new OneHotEncoder().setInputCol(e + \"_Index\").setOutputCol(e + \"_Vec\"))\n",
    "\n",
    "\n",
    "// columns that need to be added to the features vector\n",
    "val cols = Array(\"Date_Num\",  \"Airline_Vec\", \"Fuel_Price\",\n",
    "    \"Departing_Port_Vec\", \"Mean_daily_wind_run_km_Depart\", \"Mean_rainfall_mm_Depart\",\n",
    "    \"Mean_number_of_days_of_rain_Depart\",\"Mean_number_of_days_>_40_Degrees_C_Depart\",\n",
    "    \"Arriving_Port_Vec\")\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "                 .setInputCols(cols)\n",
    "                 .setOutputCol(\"indexedFeatures\"))\n",
    "\n",
    "// Choose linear combinations of explanatory variables that explain the most variance in the training data\n",
    "val pca = new PCA()\n",
    "    .setInputCol(assembler.getOutputCol)\n",
    "        //\"indexedFeatures\")\n",
    "    .setOutputCol(\"features\").setK(9)\n",
    "\n",
    "// Train a Logistic Regression model.\n",
    "val lr = new LogisticRegression()\n",
    "        .setStandardization(true)\n",
    "        .setLabelCol(\"label\")\n",
    "        .setFeaturesCol(\"features\")\n",
    "\n",
    "// Print out the parameters, documentation, and any default values.\n",
    "println(s\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\")\n",
    "\n",
    "//////////////////////////////////////////////\n",
    "//   Define and construct the ML Pipeline  ///\n",
    "//////////////////////////////////////////////\n",
    "\n",
    "val stages: Array[PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler, pca, lr)\n",
    "//val stages: Array[PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler, lr)\n",
    "\n",
    "// build the pipeline\n",
    "val pipeline = new Pipeline().setStages(stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Logistic Regression Pipeline using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.01))\n",
    "  .addGrid(lr.threshold, (for (i <- 45 to 55) yield i.toFloat / 100).toArray)\n",
    "  .addGrid(lr.tol, Array(0.000001))\n",
    "  .addGrid(lr.elasticNetParam, Array(0.0))\n",
    "  .build()\n",
    "\n",
    "// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "// This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "// is areaUnderROC.\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(10)  // Use 3+ in practice\n",
    "  //.setParallelism(2)  // Evaluate up to 2 parameter settings in parallel\n",
    "\n",
    "\n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val model = cv.fit(training)\n",
    "\n",
    "// Print the coefficients and intercept for logistic regression\n",
    "//println(s\"*******************************************\\nCoefficients: ${model.coefficients} Intercept: ${model.intercept}\")\n",
    "// Since model is a Model (i.e., a Transformer produced by an Estimator),\n",
    "// we can view the parameters it used during fit().\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this LogisticRegression instance.\n",
    "\n",
    "println(s\"ParamMap: ${model.parent.extractParamMap}\")\n",
    "\n",
    "//Get Coeffs of the Best Logistic Regression Model\n",
    "val bestModel = model.bestModel match {\n",
    "  case pm: PipelineModel => Some(pm)\n",
    "  case _ => None\n",
    "}\n",
    "\n",
    "val ml = bestModel\n",
    "  .map(_.stages.collect { case ml: LogisticRegressionModel => ml })\n",
    "  .flatMap(_.headOption)\n",
    "\n",
    "// Get best CV Model Parameters\n",
    "print(\"Intercept: \")\n",
    "println(ml.map(m => (m.intercept)).get)\n",
    "print(\"Coefficients: \")\n",
    "println(ml.map(m => (m.coefficients)).get)\n",
    "print(\"ElasticNetParam: \")\n",
    "println(ml.map(m => m.getElasticNetParam).get)\n",
    "print(\"RegParam: \")\n",
    "println(ml.map(m => m.getRegParam).get)\n",
    "print(\"Threshold: \")\n",
    "println(ml.map(m => m.getThreshold).get)\n",
    "\n",
    "// Get Results on Test Set\n",
    "val results = model.transform(testing)\n",
    "\n",
    "// Take a look at the predictions\n",
    "results.select (\"features\", \"probability\", \"prediction\", \"label\").show(10)\n",
    "\n",
    "// Measure the quality of the predictions\n",
    "getConfusionMatrix(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store The Best CV Trained Logistic Model for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Persist the Model\n",
    "//model.write.overwrite().save(\"./flightDelayModel/\")\n",
    "//val results: DataFrame = CrossValidatorModel\n",
    "//  .load(\"./flightDelayModel/\")\n",
    "//  .transform(testing)\n",
    "//  .select(\n",
    "//    col(\"features\"),\n",
    "//    col(\"label\"),\n",
    "//    col(\"prediction\")\n",
    "//  )\n",
    "\n",
    "//results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Logistic Regression Pipleine using a Train - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.01))\n",
    "  .addGrid(lr.threshold, (for (i <- 45 to 55) yield i.toFloat / 100).toArray)\n",
    "  .addGrid(lr.tol, Array(0.000001))\n",
    "  .addGrid(lr.elasticNetParam, Array(0.0,0.1))\n",
    "  .build()\n",
    "\n",
    "val tvs = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.75)\n",
    "\n",
    "//Train the Model\n",
    "val model = tvs.fit(training)\n",
    "\n",
    "println(s\"***model was fit using parameters: ${model.parent.extractParamMap}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get Results on Test Set\n",
    "val results = model.transform(testing.sample(false,0.1))\n",
    "\n",
    "// Make Predictions on the Test Dataset\n",
    "val predictions = results.select (\"features\", \"label\", \"prediction\")\n",
    "\n",
    "\n",
    "getConfusionMatrix(predictions)\n",
    "predictions.show(10)\n",
    "\n",
    "//Get Coeffs of the Best Logistic Regression Model\n",
    "val bestModel = model.bestModel match {\n",
    "  case pm: PipelineModel => Some(pm)\n",
    "  case _ => None\n",
    "}\n",
    "\n",
    "val ml = bestModel\n",
    "  .map(_.stages.collect { case ml: LogisticRegressionModel => ml })\n",
    "  .flatMap(_.headOption)\n",
    "\n",
    "print(\"Intercept: \")\n",
    "println(ml.map(m => (m.intercept)).get)\n",
    "print(\"Coefficients: \")\n",
    "println(ml.map(m => (m.coefficients)).get)\n",
    "print(\"ElasticNetParam: \")\n",
    "println(ml.map(m => m.getElasticNetParam).get)\n",
    "print(\"RegParam: \")\n",
    "println(ml.map(m => m.getRegParam).get)\n",
    "print(\"Threshold: \")\n",
    "println(ml.map(m => m.getThreshold).get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import spark.implicits._\n",
    "//import org.apache.spark.sql._\n",
    "//case class cls_Employee(name:String, sector:String, age:Int)\n",
    "//val df = Seq(\n",
    "//    cls_Employee(\"Andy\",\"aaa\", 20), \n",
    "//    cls_Employee(\"Berta\",\"bbb\", 30), \n",
    "//    cls_Employee(\"Joe\",\"ccc\", 40)\n",
    "//).toDF()\n",
    "\n",
    "//df.as[cls_Employee]\n",
    "//    .take(df.count.toInt)\n",
    "//    .foreach(t => println(s\"name=${t.name}, sector=${t.sector}, age=${t.age}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Train a DecisionTree model.\n",
    "val dt = new DecisionTreeClassifier()\n",
    "        .setLabelCol(\"label\")\n",
    "        .setFeaturesCol(\"features\")\n",
    "        .setMaxDepth(5)\n",
    "println(s\"DecisionTree parameters:\\n ${dt.explainParams()}\\n\")\n",
    "\n",
    "val stages: Array[PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler, pca, dt)\n",
    "// build the pipeline\n",
    "val dtPipeline = new Pipeline().setStages(stages)\n",
    "\n",
    "// build the pipeline\n",
    "val dtModel = dtPipeline.fit(training)\n",
    "val dtPredictions = dtModel.transform(testing)\n",
    "dtPredictions.select(\"prediction\", \"label\", \"features\").show(20)\n",
    "getConfusionMatrix(dtPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//https://docs.databricks.com/applications/machine-learning/mllib/decision-trees.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import the ML algorithms we will use.\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.Pipeline\n",
    "// StringIndexer: Read input column \"label\" (digits) and annotate them as categorical values.\n",
    "val indexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\")\n",
    "// DecisionTreeClassifier: Learn to predict column \"indexedLabel\" using the \"features\" column.\n",
    "val dtc = new DecisionTreeClassifier().setLabelCol(\"indexedLabel\")\n",
    "// Chain indexer + dtc together into a single ML Pipeline.\n",
    "val pipeline = new Pipeline().setStages(Array(indexer, dtc))\n",
    "val model = pipeline.fit(training)\n",
    "// The tree is the last stage of the Pipeline.  Display it!\n",
    "val tree = model.stages.last.asInstanceOf[DecisionTreeClassificationModel]\n",
    "\n",
    "val variedMaxDepthModels = (0 until 8).map { maxDepth =>\n",
    "  // For this setting of maxDepth, learn a decision tree.\n",
    "  dtc.setMaxDepth(maxDepth)\n",
    "  // Create a Pipeline with our feature processing stage (indexer) plus the tree algorithm\n",
    "  val pipeline = new Pipeline().setStages(Array(indexer, dtc))\n",
    "  // Run the ML Pipeline to learn a tree.\n",
    "  pipeline.fit(training)\n",
    "}\n",
    "// Define an evaluation metric.  In this case, we will use \"weightedPrecision\", which is equivalent to 0-1 accuracy.\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setMetricName(\"weightedPrecision\")\n",
    "// For each maxDepth setting, make predictions on the test data, and compute the accuracy metric.\n",
    "val accuracies = (0 until 8).map { maxDepth =>\n",
    "  val model = variedMaxDepthModels(maxDepth)\n",
    "  // Calling transform() on the test set runs the fitted pipeline.\n",
    "  // The learned model makes predictions on each test example.\n",
    "  val predictions = model.transform(test)\n",
    "  // Calling evaluate() on the predictions DataFrame computes our accuracy metric.\n",
    "  (maxDepth, evaluator.evaluate(predictions))\n",
    "}.toDF(\"maxDepth\", \"accuracy\")\n",
    "\n",
    "accuracies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Train a GBT model.\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setMaxIter(10)\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "println(s\"DecisionTree parameters:\\n ${gbt.explainParams()}\\n\")\n",
    "\n",
    "val stages: Array[PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler, pca, gbt)\n",
    "// build the pipeline\n",
    "val gbtPipeline = new Pipeline().setStages(stages)\n",
    "\n",
    "// build the pipeline\n",
    "val gbtModel = gbtPipeline.fit(training)\n",
    "val gbtPredictions = gbtModel.transform(testing.sample(false,0.1))\n",
    "gbtPredictions.select(\"prediction\", \"label\", \"features\").show(20)\n",
    "getConfusionMatrix(gbtPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPR_FPR DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val variedThresholdModels = (0 to 20).map { threshold => \n",
    "    lr.setThreshold(threshold * 0.05)      \n",
    "    val stages: Array[PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler, pca, lr)\n",
    "    // build the pipeline\n",
    "    val pipeline = new Pipeline().setStages(stages)\n",
    "    pipeline.fit(training)\n",
    "}\n",
    "\n",
    "// Define an evaluation metric.  In this case, we will use \"weightedPrecision\", which is equivalent to 0-1 accuracy.\n",
    "//import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "//val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"weightedPrecision\")\n",
    "// For each threshold setting, make predictions on the test data, and compute the accuracy metric.\n",
    "val accuracies = (0 to 20).map { threshold =>\n",
    "    val model = variedThresholdModels(threshold)\n",
    "    // Calling transform() on the test set runs the fitted pipeline.\n",
    "    // The learned model makes predictions on each test example.\n",
    "    val predictions = model.transform(testing.sample(false,0.1))\n",
    "    // Calling evaluate() on the predictions DataFrame computes our accuracy metric.\n",
    "    val TP = predictions.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "    val TN = predictions.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "    val FP = predictions.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "    val FN = predictions.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "    val TPR = TP/(1D* (TP+FN))\n",
    "    val FPR = FP/(1D* (FP+TN))\n",
    "    (threshold * 0.05, TP, TN, FP, FN, FPR, TPR)\n",
    "}.toDF(\"threshold\", \"TP\", \"TN\", \"FP\", \"FN\", \"FPR\", \"TPR\")\n",
    "\n",
    "\n",
    "accuracies.show()\n",
    "\n",
    "//Write ROC data to file\n",
    "accuracies\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"sep\",\",\")\n",
    "    .mode(\"overwrite\")\n",
    "    .csv(\"/Users/tod/Documents/_GDDS/_FIT5202/big_data/FlightData/ROC.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
