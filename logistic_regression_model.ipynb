{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 8 Assignment Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://22ef46be2451:4042\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590568975015)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors, Matrix, Matrices}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\n",
       "import org.apache.log4j._\n",
       "spark: org.apa..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Start a simple Spark Session\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "//Import VectorAssembler for the Feature Vector and Linear Algebra data structures\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors, Matrix, Matrices}\n",
    "\n",
    "//Model Building Pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "//Binary Classification\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel,\n",
    "                                           RandomForestClassifier, GBTClassifier,DecisionTreeClassifier}\n",
    "//Model Training\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "\n",
    "//Model Evaluation\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "\n",
    "//Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "//For Cleaning\n",
    "//import scala.util.matching.Regex\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a parquet file of flight delay, fuel-price and meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Departing_Port: string (nullable = true)\n",
      " |-- Arriving_Port: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month_Num: string (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- Departing_Port_station_ID: string (nullable = true)\n",
      " |-- Departing_Port_station_name: string (nullable = true)\n",
      " |-- Arriving_Port_station_ID: string (nullable = true)\n",
      " |-- Arriving_Port_station_name: string (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Depart: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Depart: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Depart: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Depart: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Depart: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Depart: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Arrive: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Arrive: double (nullable = true)\n",
      " |-- Date_Num: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampleFraction: Double = 0.1\n",
       "flights: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Take a random sample (without replacement) of the data (to reduce memory requirements)\n",
    "val sampleFraction = 0.1\n",
    "\n",
    "//Concatenate rows df2 and df3 and drop any rows with missing data\n",
    "val flights = (spark\n",
    "            .read.parquet(\"flightDelay.parquet\")\n",
    "            .withColumn(\"Month_Num1\", $\"Month_Num\" cast \"Int\")\n",
    "            .withColumn(\"Date_Num\",  ($\"Year\"-2004)*12 + $\"Month_Num1\")\n",
    "            .drop(\"Sectors_Flown\", \"Month_Num1\", \"Change\")\n",
    "            .withColumnRenamed(\"Departures_Delayed\",\"label\")\n",
    "            .withColumnRenamed(\"Price\",\"Fuel_Price\")\n",
    "            .sample(false,sampleFraction)\n",
    "            .na.drop())\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the proportion of lates in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of lates (label=1) in the sample\n",
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|107060|\n",
      "|    0|523272|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counts: org.apache.spark.sql.DataFrame = [label: int, count: bigint]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counts = flights.groupBy(\"label\").count()\n",
    "\n",
    "println(\"proportion of lates (label=1) in the sample\")\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a Logistic Regression Pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      " aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true, current: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoricalVariables: Array[String] = Array(Departing_Port, Arriving_Port, Airline)\n",
       "categoricalIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_ec63a14ad65b, strIdx_4310e2c153c8, strIdx_0aa7c51cd43a)\n",
       "categoricalEncoders: Array[org.apache.spark.ml.feature.OneHotEncoder] = Array(oneHot_ff870cf72ebc, oneHot_5d0f501b7d5c, oneHot_8da77fec4e2b)\n",
       "cols: Array[String] = Array(Date_Num, Airline_Vec, Fuel_Price, Departing_Port_Vec, Mean_9am_wind_speed_km/h_Depart, Mean_rainfall_mm_Depart, Arriving_Port_Vec, Mean_9am_wind_speed_km/h_Arrive, Mean_rainfall_mm_Arrive)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_984952bfeaf0\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_eb9afb89e131\n",
       "stages: Array[org.apache.spark.ml.PipelineStage] =..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//////////////////////////////////////////////////\n",
    "//// Setting Up DataFrame for Machine Learning ///\n",
    "//////////////////////////////////////////////////\n",
    "\n",
    "// Deal with Categorical Columns\n",
    "val categoricalVariables = Array(\n",
    "    \"Departing_Port\", \"Arriving_Port\", \"Airline\")\n",
    "val categoricalIndexers = categoricalVariables\n",
    "  .map(i => new StringIndexer().setInputCol(i).setOutputCol(i+\"_Index\"))\n",
    "val categoricalEncoders = categoricalVariables\n",
    "  .map(e => new OneHotEncoder().setInputCol(e + \"_Index\").setOutputCol(e + \"_Vec\"))\n",
    "\n",
    "\n",
    "// columns that need to added to the features vector\n",
    "val cols = Array(\"Date_Num\",  \"Airline_Vec\", \"Fuel_Price\",\n",
    "    \"Departing_Port_Vec\", \"Mean_9am_wind_speed_km/h_Depart\", \"Mean_rainfall_mm_Depart\",\n",
    "    \"Arriving_Port_Vec\", \"Mean_9am_wind_speed_km/h_Arrive\",\"Mean_rainfall_mm_Arrive\")\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "                 .setInputCols(cols)\n",
    "                 .setOutputCol(\"features\") )\n",
    "\n",
    "\n",
    "/////////////////////////////\n",
    "// Set Up the Pipeline //////\n",
    "/////////////////////////////\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "        //.setFitIntercept(true)\n",
    "        //.setRegParam(0.01)\n",
    "        .setMaxIter(100)\n",
    "        //.setTol(0.001)\n",
    "        //.setThreshold(0.2)\n",
    "        .setStandardization(true)\n",
    "        //.setWeightCol(\"classWeightCol\")\n",
    "        .setLabelCol(\"label\")\n",
    "        .setFeaturesCol(\"features\")\n",
    "        //.setFamily(\"multinomial\")\n",
    "\n",
    "// Print out the parameters, documentation, and any default values.\n",
    "println(s\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\")\n",
    "//val stages = Array(departureIndexer,arrivalIndexer,airlineIndexer,\n",
    "//          departureEncoder,arrivalEncoder, airlineEncoder, \n",
    "//          assembler, lr)\n",
    "\n",
    "val stages: Array[org.apache.spark.ml.PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler, lr)\n",
    "\n",
    "\n",
    "\n",
    "// build the pipeline\n",
    "val pipeline = new Pipeline().setStages(stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pipleine using a Train - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-27 07:47:20,552 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-05-27 07:47:20,553 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "***model was fit using parameters: {\n",
      "\ttvs_8154792a39a5-collectSubModels: false,\n",
      "\ttvs_8154792a39a5-estimator: pipeline_134793f1313b,\n",
      "\ttvs_8154792a39a5-estimatorParamMaps: [Lorg.apache.spark.ml.param.ParamMap;@f9facf,\n",
      "\ttvs_8154792a39a5-evaluator: binEval_14deb6311477,\n",
      "\ttvs_8154792a39a5-parallelism: 1,\n",
      "\ttvs_8154792a39a5-seed: -1772833110,\n",
      "\ttvs_8154792a39a5-trainRatio: 0.75\n",
      "}\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.0,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 1.0,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-tol: 0.001\n",
       "}, {\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.5,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 1.0,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-tol: 0.001\n",
       "}, {\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 1.0,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 1.0,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-tol: 0.001\n",
       "}, {\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.0,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 1.0,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.2,\n",
       "\tlogreg_1f564ab5e3fc-tol: 0.001\n",
       "}, {\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.5,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 1.0,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.2,\n",
       "\tlogre..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(1.0, 0.3, 0.2, 0.1))\n",
    "  .addGrid(lr.threshold, Array(0.1,0.2,0.3,0.4,0.5))\n",
    "  .addGrid(lr.tol, Array(0.001,0.00001))\n",
    "  .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "  .build()\n",
    "\n",
    "val tvs = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline) // the estimator can also just be an individual model rather than a pipeline\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.75)\n",
    "\n",
    "//////////////////////////\n",
    "/// Split the Data ///////\n",
    "//////////////////////////\n",
    "val Array(training, test) = flights.randomSplit(Array(0.7, 0.3), seed = 12345)\n",
    "\n",
    "//Create a holdout test set\n",
    "val model = tvs.fit(training)\n",
    "\n",
    "println(s\"***model was fit using parameters: ${model.parent.extractParamMap}\")\n",
    "\n",
    "\n",
    "// Get Results on Test Set\n",
    "val results = model.transform(test)\n",
    "\n",
    "// Make Predictions on the Test Dataset\n",
    "val predictions = results.select (\"features\", \"label\", \"prediction\")\n",
    "\n",
    "\n",
    "\n",
    "predictions.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Train - Validation Split Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8291708502880093\n",
      "Precision: 0.7851474997823255\n",
      "Recall: 0.8291708502880093\n",
      "F1: 0.7517689638502538\n",
      "Confusion matrix\n",
      "(Predict N, Predict P):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_8d634c28e750\n",
       "TP: Long = 4\n",
       "TN: Long = 156900\n",
       "FP: Long = 3\n",
       "FN: Long = 32323\n",
       "total: Double = 189230.0\n",
       "confusion: org.apache.spark.ml.linalg.Matrix =\n",
       "156900.0  32323.0\n",
       "3.0       4.0\n",
       "accuracy: Double = 0.8291708502880093\n",
       "precision: Double = 3.699202029276542E-5\n",
       "recall: Double = 0.17083443428631823\n",
       "F1: Double = 7.39680237523491E-5\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "println(s\"Accuracy: ${eval.setMetricName(\"accuracy\").evaluate(results)}\")\n",
    "println(s\"Precision: ${eval.setMetricName(\"weightedPrecision\").evaluate(results)}\")\n",
    "println(s\"Recall: ${eval.setMetricName(\"weightedRecall\").evaluate(results)}\")\n",
    "println(s\"F1: ${eval.setMetricName(\"f1\").evaluate(results)}\")\n",
    "\n",
    "val TP = results.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "val TN = results.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "val FP = results.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "val FN = results.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "val total = results.select(\"label\").count.toDouble\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix\\n(Predict N, Predict P):\")\n",
    "val confusion: Matrix = Matrices.dense(2, 2, Array(TN, FP, FN, TP))\n",
    "\n",
    "val accuracy    = (TP + TN) / total\n",
    "val precision   = (TP + FP) / total\n",
    "val recall      = (TP + FN) / total\n",
    "val F1 = 2/(1/precision + 1/recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Pipeline using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***model was fit using parameters: {\n",
      "\tcv_9a008cc8a679-collectSubModels: false,\n",
      "\tcv_9a008cc8a679-estimator: pipeline_134793f1313b,\n",
      "\tcv_9a008cc8a679-estimatorParamMaps: [Lorg.apache.spark.ml.param.ParamMap;@4d3cdade,\n",
      "\tcv_9a008cc8a679-evaluator: binEval_2e6aec4b6d1b,\n",
      "\tcv_9a008cc8a679-numFolds: 5,\n",
      "\tcv_9a008cc8a679-parallelism: 2,\n",
      "\tcv_9a008cc8a679-seed: -1191137437\n",
      "}\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "|(101,[0,4,10,14,5...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.0,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.19,\n",
       "\tlogreg_1f564ab5e3fc-tol: 1.0E-6\n",
       "}, {\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.0,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.2,\n",
       "\tlogreg_1f564ab5e3fc-tol: 1.0E-6\n",
       "}, {\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.0,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.21,\n",
       "\tlogreg_1f564ab5e3fc-tol: 1.0E-6\n",
       "}, {\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.0,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.22,\n",
       "\tlogreg_1f564ab5e3fc-tol: 1.0E-6\n",
       "})\n",
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_9a008cc8a679\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spa..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.1))\n",
    "  .addGrid(lr.threshold, Array(0.19,0.20,0.21,0.22))\n",
    "  .addGrid(lr.tol, Array(0.000001))\n",
    "  .addGrid(lr.elasticNetParam, Array(0.0))\n",
    "  .build()\n",
    "\n",
    "// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "// This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "// is areaUnderROC.\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(5)  // Use 3+ in practice\n",
    "  .setParallelism(2)  // Evaluate up to 2 parameter settings in parallel\n",
    "\n",
    "\n",
    "\n",
    "//////////////////////////\n",
    "/// Split the Data ///////\n",
    "//////////////////////////\n",
    "val Array(training, test) = flights.randomSplit(Array(0.7, 0.3), seed = 12345)\n",
    "\n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val model = cv.fit(training)\n",
    "\n",
    "// Print the coefficients and intercept for logistic regression\n",
    "//println(s\"*******************************************\\nCoefficients: ${model.coefficients} Intercept: ${model.intercept}\")\n",
    "// Since model is a Model (i.e., a Transformer produced by an Estimator),\n",
    "// we can view the parameters it used during fit().\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this LogisticRegression instance.\n",
    "\n",
    "println(s\"***model was fit using parameters: ${model.parent.extractParamMap}\")\n",
    "\n",
    "// Get Results on Test Set\n",
    "val results = model.transform(test)\n",
    "\n",
    "results.select (\"features\", \"label\", \"prediction\").show(10)\n",
    "\n",
    "//val checkResults = results\n",
    "//  .select(\"features\", \"label\", \"myProbability\", \"prediction\")\n",
    "//  .collect()\n",
    "//  .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>\n",
    "//    println(s\"($features, $label) -> prob=$prob, prediction=$prediction\")\n",
    "//  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Logistic Regression Coeffs of the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestModel: Option[org.apache.spark.ml.PipelineModel] = Some(pipeline_134793f1313b)\n",
       "lrm: Option[org.apache.spark.ml.classification.LogisticRegressionModel] = Some(LogisticRegressionModel: uid = logreg_1f564ab5e3fc, numClasses = 2, numFeatures = 101)\n",
       "res3: Option[(Double, org.apache.spark.ml.linalg.Vector)] = Some((-2.189761629189335,[9.414462824977844E-4,-0.08140327587296356,-0.06924183934886433,0.016578044795738763,0.2173017236217448,-0.149566406056975,0.2459800966983625,-0.05625479194480765,0.10095811521141998,0.14755353591339437,0.14814834202438507,0.07334155225120798,0.01655680132441062,-0.0549806754929559,-0.15427974683060613,0.004772328380276604,-0.14482221975516527,0.08096921806551298,-0.074896687315277,0.12894386365721228,-0.06854959477386152,0.09652294406944058,0.051970878317700..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bestModel = model.bestModel match {\n",
    "  case pm: PipelineModel => Some(pm)\n",
    "  case _ => None\n",
    "}\n",
    "\n",
    "val lrm = bestModel\n",
    "  .map(_.stages.collect { case lrm: LogisticRegressionModel => lrm })\n",
    "  .flatMap(_.headOption)\n",
    "\n",
    "lrm.map(m => (m.intercept, m.coefficients))\n",
    "//lrm.map(m => (m.summary.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Parameters of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Option[org.apache.spark.ml.param.ParamMap] =\n",
       "Some({\n",
       "\tlogreg_1f564ab5e3fc-aggregationDepth: 2,\n",
       "\tlogreg_1f564ab5e3fc-elasticNetParam: 0.0,\n",
       "\tlogreg_1f564ab5e3fc-family: auto,\n",
       "\tlogreg_1f564ab5e3fc-featuresCol: features,\n",
       "\tlogreg_1f564ab5e3fc-fitIntercept: true,\n",
       "\tlogreg_1f564ab5e3fc-labelCol: label,\n",
       "\tlogreg_1f564ab5e3fc-maxIter: 100,\n",
       "\tlogreg_1f564ab5e3fc-predictionCol: prediction,\n",
       "\tlogreg_1f564ab5e3fc-probabilityCol: probability,\n",
       "\tlogreg_1f564ab5e3fc-rawPredictionCol: rawPrediction,\n",
       "\tlogreg_1f564ab5e3fc-regParam: 0.1,\n",
       "\tlogreg_1f564ab5e3fc-standardization: true,\n",
       "\tlogreg_1f564ab5e3fc-threshold: 0.19,\n",
       "\tlogreg_1f564ab5e3fc-tol: 1.0E-6\n",
       "})\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.map(m => m.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7107118321619194\n",
      "Precision: 0.7459253241743581\n",
      "Recall: 0.7107118321619194\n",
      "F1: 0.7264223031692968\n",
      "Confusion matrix\n",
      "(Predict N, Predict P):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_f0ce86e6f9a0\n",
       "TP: Long = 10427\n",
       "TN: Long = 124061\n",
       "FP: Long = 32842\n",
       "FN: Long = 21900\n",
       "total: Double = 189230.0\n",
       "confusion: org.apache.spark.ml.linalg.Matrix =\n",
       "124061.0  21900.0\n",
       "32842.0   10427.0\n",
       "accuracy: Double = 0.7107118321619194\n",
       "precision: Double = 0.22865824657823813\n",
       "recall: Double = 0.17083443428631823\n",
       "F1: Double = 0.19556154127558875\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "println(s\"Accuracy: ${eval.setMetricName(\"accuracy\").evaluate(results)}\")\n",
    "println(s\"Precision: ${eval.setMetricName(\"weightedPrecision\").evaluate(results)}\")\n",
    "println(s\"Recall: ${eval.setMetricName(\"weightedRecall\").evaluate(results)}\")\n",
    "println(s\"F1: ${eval.setMetricName(\"f1\").evaluate(results)}\")\n",
    "\n",
    "val TP = results.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "val TN = results.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "val FP = results.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "val FN = results.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "val total = results.select(\"label\").count.toDouble\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix\\n(Predict N, Predict P):\")\n",
    "val confusion: Matrix = Matrices.dense(2, 2, Array(TN, FP, FN, TP))\n",
    "\n",
    "val accuracy    = (TP + TN) / total\n",
    "val precision   = (TP + FP) / total\n",
    "val recall      = (TP + FN) / total\n",
    "val F1 = 2/(1/precision + 1/recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
